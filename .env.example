# ============================================================================
# AZURE OPENAI CONFIGURATION
# ============================================================================

# [Required] Azure OpenAI endpoint URL
# Format: https://<resource-name>.openai.azure.com/
# Find in Azure Portal: Search "OpenAI" > Your resource > Endpoints
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/

# [Required] Azure OpenAI API key for authentication
# Format: 32-character alphanumeric string
# Find in Azure Portal: OpenAI resource > Keys and Endpoints > Key 1 or Key 2
# IMPORTANT: Keep this secret! Don't commit to version control.
AZURE_OPENAI_API_KEY=your-api-key-here

# [Required] Name of your deployed model in Azure
# Examples: gpt-4, gpt-4-turbo, gpt-35-turbo, gpt-4o
# Find in Azure Portal: OpenAI resource > Deployments
AZURE_OPENAI_DEPLOYMENT_NAME=gpt-4o

# [Optional] Azure OpenAI API version
# This controls which API features are available
# See: https://learn.microsoft.com/en-us/azure/ai-services/openai/reference
# Default: 2024-08-01-preview
# AZURE_OPENAI_API_VERSION=2024-08-01-preview

# [Optional] Temperature: Controls response creativity
# Range: 0.0 (deterministic) to 2.0 (creative)
# Default: 0.0 (best for troubleshooting)
# LLM_TEMPERATURE=0.0

# [Optional] Maximum tokens in LLM response
# Range: 1 to model max (usually 4096 or 8192)
# Default: 2048
# LLM_MAX_TOKENS=2048

# ============================================================================
# PROMETHEUS CONFIGURATION
# ============================================================================

# [Required] Prometheus server URL for metrics queries
# Format: http://host:port or https://host:port
# Examples:
#   - Local Kubernetes: http://prometheus.monitoring:9090
#   - Docker Compose: http://prometheus:9090
#   - External: https://prometheus.example.com:9090
PROMETHEUS_URL=http://prometheus-server.monitoring-system:80

# [Optional] Prometheus API request timeout in seconds
# Default: 10
# PROMETHEUS_TIMEOUT=10

# [Optional] Maximum number of time series to return from metric queries
# Prometheus can return thousands of metrics; this limits display
# Default: 20
# PROMETHEUS_MAX_SERIES_RESULTS=20

# ============================================================================
# EMAIL CONFIGURATION
# ============================================================================
# Email notifications are always sent after each investigation.

# [Required] SMTP server hostname
# Examples: smtp.gmail.com, smtp.office365.com, smtp.sendgrid.net
SMTP_HOST=smtp.gmail.com

# [Required] Sender email address
# Must be a valid email address authorized to send from your SMTP server
EMAIL_FROM=kube-medic@example.com

# [Required] Recipient email address
# All investigation reports will be sent to this address
EMAIL_TO=ops-team@example.com

# [Optional] SMTP server port
# Common values: 587 (STARTTLS), 465 (SSL/TLS), 25 (unencrypted)
# Default: 587
# SMTP_PORT=587

# [Optional] SMTP authentication username
# Usually your email address or API username
# SMTP_USERNAME=your-email@example.com

# [Optional] SMTP authentication password
# For Gmail, use an "App Password" (not your regular password)
# For SendGrid/SES, use your API key
# IMPORTANT: Keep this secret! Don't commit to version control.
# SMTP_PASSWORD=your-smtp-password

# [Optional] Use TLS for SMTP connection
# Set to false only if using port 465 (SSL) or testing locally
# Default: true
# SMTP_USE_TLS=true

# ============================================================================
# KUBERNETES CONFIGURATION
# ============================================================================
# Cluster configuration is read from ~/.kube/config or in-cluster config.

# [Optional] Number of log lines to retrieve from pod logs per request
# Default: 50
# K8S_LOGS_TAIL_LINES=50

# [Optional] Maximum characters to display from pod logs
# Prevents overwhelming the LLM with huge logs
# Default: 3000
# K8S_LOGS_MAX_CHARS=3000

# ============================================================================
# API SERVER CONFIGURATION
# ============================================================================

# [Optional] Host to bind the API server to
# 0.0.0.0 = all interfaces, 127.0.0.1 = localhost only
# Default: 0.0.0.0
# API_HOST=0.0.0.0

# [Optional] Port to run the API server on
# Default: 8000
# API_PORT=8000

# [Optional] API server log level
# Options: debug, info, warning, error, critical
# Default: info
# API_LOG_LEVEL=info

# ============================================================================
# LOGGING CONFIGURATION
# ============================================================================

# [Optional] Logging level
# Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
# Default: INFO
# LOG_LEVEL=INFO

# [Optional] Path to save logs to a file
# If not set, logs go to console only
# LOG_FILE=logs/kube-medic.log

# [Optional] Log format style
# Options: simple ("LEVEL | message"), detailed (with timestamp and module)
# Default: detailed
# LOG_FORMAT=detailed

# ============================================================================
# TEXT FORMATTING
# ============================================================================

# [Optional] Default maximum length for text truncation
# Default: 500
# TEXT_TRUNCATE_MAX_LENGTH=500
