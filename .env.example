# =========================================================================
# REQUIRED SETTINGS (no defaults = must be set)
# =========================================================================
# These variables MUST be configured before running KubeMedic.
# The application will fail to start if any of these are missing.

# Azure OpenAI endpoint URL
# Format: https://<resource-name>.openai.azure.com/
# Find in Azure Portal: Search "OpenAI" > Your resource > Endpoints
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/

# Azure OpenAI API key for authentication
# Format: 32-character alphanumeric string
# Find in Azure Portal: OpenAI resource > Keys and Endpoints > Key 1 or Key 2
# IMPORTANT: Keep this secret! Don't commit to version control.
AZURE_OPENAI_API_KEY=your-api-key-here

# Name of your deployed model in Azure
# Examples: gpt-4, gpt-4-turbo, gpt-35-turbo, gpt-4o
# Find in Azure Portal: OpenAI resource > Deployments
AZURE_OPENAI_DEPLOYMENT_NAME=gpt-4o

# Prometheus server URL for metrics queries
# Format: http://host:port or https://host:port
# Examples:
#   - Local Kubernetes: http://prometheus.monitoring:9090
#   - Docker Compose: http://prometheus:9090
#   - External: https://prometheus.example.com:9090
# Note: Trailing slash will be automatically removed
PROMETHEUS_URL=http://prometheus-server.monitoring-system:80

# ============================================================================
# LLM CONFIGURATION (Optional)
# ============================================================================
# These settings control the behavior of the Azure OpenAI language model.
# Reasonable defaults are already set in the code; uncomment to override.

# Azure OpenAI API version
# This controls which API features are available
# Current recommended: 2024-08-01-preview (stable and feature-rich)
# See: https://learn.microsoft.com/en-us/azure/ai-services/openai/reference
# AZURE_OPENAI_API_VERSION=2024-08-01-preview

# Temperature: Controls response creativity
# Range: 0.0 (deterministic) to 2.0 (creative)
# Recommended values:
#   0.0   = Precise, consistent answers (best for troubleshooting)
#   0.5   = Balanced mix of creativity and consistency
#   1.0   = Creative, varied responses
#   1.5+  = Very creative, unpredictable
# Default: 0.0 (production recommended)
# LLM_TEMPERATURE=0.0

# Maximum tokens in LLM response
# Range: 1 to model max (usually 4096 or 8192)
# Higher values allow longer responses but use more tokens/cost
# Typical usage:
#   512   = Short, concise responses
#   1024  = Medium responses with some detail
#   2048  = Longer responses with full context (default, recommended)
#   4096+ = Maximum detail (costs more, slower)
# Default: 2048 (good balance)
# LLM_MAX_TOKENS=2048

# ============================================================================
# PROMETHEUS CONFIGURATION (Optional)
# ============================================================================
# These settings control how KubeMedic queries your Prometheus server.
# Reasonable defaults are already set; uncomment to override.

# Prometheus API request timeout in seconds
# Set higher if your Prometheus server is slow or far away
# Recommended values:
#   5    = Fast networks, responsive server
#   10   = Standard (default, production recommended)
#   20   = Slow networks or overloaded servers
#   30+  = Very unreliable networks
# Default: 10 seconds
# PROMETHEUS_TIMEOUT=10

# Maximum number of time series to return from metric queries
# Prometheus can return thousands of metrics; this limits display
# Recommended values:
#   10   = Only show top results (fast, clean)
#   20   = Show top results with context (default, recommended)
#   50   = Show detailed results
#   100+ = Show everything (may overwhelm LLM context)
# Default: 20 (good balance)
# PROMETHEUS_MAX_SERIES_RESULTS=20

# ============================================================================
# KUBERNETES CONFIGURATION (Optional)
# ============================================================================
# These settings control how KubeMedic interacts with Kubernetes.
# Configuration is read from ~/.kube/config or in-cluster config automatically.

# Number of log lines to retrieve from pod logs per request
# Set higher to get more context from logs
# Recommended values:
#   20   = Minimal logs, fast
#   50   = Standard amount of context (default, recommended)
#   100  = More detail from logs
#   200+ = Very detailed log context
# Default: 50 lines
# K8S_LOGS_TAIL_LINES=50

# Maximum characters to display from pod logs
# Prevents overwhelming the LLM with huge logs
# Recommended values:
#   1000  = Compact log display
#   3000  = Standard context (default, recommended)
#   5000  = More log context
#   10000 = Very detailed logs
# Note: Each log retrieval is limited to prevent token waste
# Default: 3000 characters (good balance)
# K8S_LOGS_MAX_CHARS=3000

# ============================================================================
# TEXT FORMATTING (Optional)
# ============================================================================
# These settings control output text formatting.

# Default maximum length for text truncation
# Used when formatting output text for display
# Recommended values:
#   200   = Very short summaries
#   500   = Standard summaries (default, recommended)
#   1000  = Detailed output
#   2000+ = Full detail
# Default: 500 characters
# TEXT_TRUNCATE_MAX_LENGTH=500

# ============================================================================
# LOGGING CONFIGURATION (Optional)
# ============================================================================
# These settings control application logging output.
# Affects visibility into what the application is doing.

# Logging level: What messages to display
# Options and when to use:
#   DEBUG    = Development/troubleshooting (shows everything)
#   INFO     = Production/normal operation (important events only)
#   WARNING  = (not recommended)
#   ERROR    = Emergency mode (errors only)
#   CRITICAL = (not recommended)
# Note: DEBUG level is verbose but very helpful for debugging
# Default: INFO (production recommended)
# LOG_LEVEL=INFO

# Path to save logs to a file (optional)
# If not set, logs go to console only
# Examples:
#   logs/kube-medic.log          = Local development
#   /var/log/kube-medic/app.log  = Production (Linux)
#   C:\\logs\\kube-medic.log     = Windows
# Note: Directory will be created automatically if it doesn't exist
# Default: (empty - console only)
# LOG_FILE=logs/kube-medic.log

# Log format style: How to format log messages
# Options:
#   simple   = "LEVEL | message" (compact)
#   detailed = "YYYY-MM-DD HH:MM:SS | module | LEVEL | message" (full context)
# Recommended:
#   simple   = For CI/CD logs or minimal output
#   detailed = For debugging and production (default, recommended)
# Default: detailed
# LOG_FORMAT=detailed

# ============================================================================
# EXAMPLE CONFIGURATIONS
# ============================================================================
# Copy and paste these examples to quickly set up for different scenarios.

# --- Development Configuration ---
# Use this when actively developing or debugging KubeMedic
# Verbose logging helps understand what's happening
# LOG_LEVEL=DEBUG
# LOG_FILE=logs/kube-medic-dev.log
# LOG_FORMAT=detailed

# --- Production Configuration ---
# Use this for production deployments
# Minimal but persistent logging for monitoring
# LOG_LEVEL=INFO
# LOG_FILE=/var/log/kube-medic/app.log
# LOG_FORMAT=detailed

# --- Quick Debugging Configuration ---
# Use this when troubleshooting a specific issue
# Console-only verbose output for interactive debugging
# LOG_LEVEL=DEBUG
# LOG_FORMAT=detailed

# --- Strict LLM Configuration ---
# Use this to minimize token usage and API costs
# Smaller responses, fewer tokens, faster processing
# LLM_TEMPERATURE=0.0
# LLM_MAX_TOKENS=1024
# PROMETHEUS_MAX_SERIES_RESULTS=10
# K8S_LOGS_TAIL_LINES=30
# K8S_LOGS_MAX_CHARS=1500

# --- Detailed Analysis Configuration ---
# Use this for comprehensive troubleshooting
# More context from all sources, detailed logging
# LOG_LEVEL=DEBUG
# LLM_MAX_TOKENS=4096
# PROMETHEUS_MAX_SERIES_RESULTS=50
# K8S_LOGS_TAIL_LINES=200
# K8S_LOGS_MAX_CHARS=5000